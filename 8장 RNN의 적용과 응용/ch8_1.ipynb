{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 모형의 비교\n",
    "IMDB 데이터\n",
    "\n",
    "- simple RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "max_features=10000 # 9999개의 단어 사용\n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(type(x_train));print(type(x_test))\n",
    "print(type(y_train));print(type(y_test))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 12500, 1: 12500}\n",
      "{0: 12500, 1: 12500}\n"
     ]
    }
   ],
   "source": [
    "# y_train과 y_test 분포\n",
    "unique,count=np.unique(y_train,return_counts=True)\n",
    "print(dict(zip(unique,count)))\n",
    "unique,count=np.unique(y_test,return_counts=True)\n",
    "print(dict(zip(unique,count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "218\n",
      "88584\n",
      "<class 'dict'>\n",
      "lacks\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(len(x_train[0]))\n",
    "wordindex=imdb.get_word_index()\n",
    "print(len(wordindex)) # 단어 수\n",
    "print(type(wordindex)) # 자료형\n",
    "\n",
    "for key,value in wordindex.items():\n",
    "    if value==1500:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1500로 인덱스된 단어는 lacks 이며 lacks는 IMDB에 있는 단어 중 1500번째로 빈도가 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ! is an amazing actor and now the same being director ! father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ! and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ! to the two little boy's that played the ! of norman and paul they were just brilliant children are often left out of the ! list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 계열을 원래 문장으로 환원\n",
    "exchindex=dict([(value,key) for (key,value) in wordindex.items()]) # 자리 바꿈(swap)\n",
    "review=' '.join([exchindex.get(i-3,'!') for i in x_train[0]]) # 계열의 시작, padding, unkown 제외\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 700)\n",
      "(25000, 700)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "lenmax=700 # 700개의 계열로 padding\n",
    "input_train=sequence.pad_sequences(x_train,maxlen=lenmax) # 패딩된 input\n",
    "input_test=sequence.pad_sequences(x_test,maxlen=lenmax)\n",
    "print(input_train.shape)\n",
    "print(input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 700, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "srnn=Sequential()\n",
    "srnn.add(Embedding(max_features,32,input_shape=(700,)))\n",
    "srnn.add(SimpleRNN(32))\n",
    "srnn.add(Dense(1,activation='sigmoid'))\n",
    "srnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "196/196 [==============================] - 58s 280ms/step - loss: 0.5839 - acc: 0.6819\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 57s 291ms/step - loss: 0.3590 - acc: 0.8522\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 56s 288ms/step - loss: 0.2938 - acc: 0.8824\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 55s 281ms/step - loss: 0.2324 - acc: 0.9114\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 54s 278ms/step - loss: 0.1993 - acc: 0.9260\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 53s 272ms/step - loss: 0.1484 - acc: 0.9461\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 55s 278ms/step - loss: 0.1105 - acc: 0.9616\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 54s 274ms/step - loss: 0.0814 - acc: 0.9719\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 54s 273ms/step - loss: 0.0585 - acc: 0.9818\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 54s 274ms/step - loss: 0.0403 - acc: 0.9872\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0135 - acc: 0.9976\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.5611 - acc: 0.8342\n",
      "[0.013456299901008606, 0.9976400136947632]\n",
      "[0.561057984828949, 0.8342000246047974]\n"
     ]
    }
   ],
   "source": [
    "srnn.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "srnn.fit(input_train,y_train,epochs=10,batch_size=128)\n",
    "\n",
    "train_loss_acc=srnn.evaluate(input_train,y_train)\n",
    "test_loss_acc=srnn.evaluate(input_test,y_test)\n",
    "\n",
    "print(train_loss_acc)\n",
    "print(test_loss_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과대적합 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 700, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 328,353\n",
      "Trainable params: 328,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 55s 275ms/step - loss: 0.5759 - acc: 0.6932\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 56s 285ms/step - loss: 0.2859 - acc: 0.8899\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 58s 294ms/step - loss: 0.2254 - acc: 0.9164\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 73s 374ms/step - loss: 0.2042 - acc: 0.9265\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 57s 292ms/step - loss: 0.1761 - acc: 0.9361\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 53s 271ms/step - loss: 0.1596 - acc: 0.9438\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 63s 320ms/step - loss: 0.1463 - acc: 0.9496\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 68s 347ms/step - loss: 0.1413 - acc: 0.9504\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 52s 267ms/step - loss: 0.1244 - acc: 0.9576\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 55s 283ms/step - loss: 0.1195 - acc: 0.9595\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1108 - acc: 0.9628\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.3842 - acc: 0.8585\n",
      "[0.1108006089925766, 0.9627599716186523]\n",
      "[0.38420921564102173, 0.8584799766540527]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "m_lstm=Sequential()\n",
    "m_lstm.add(Embedding(max_features,32, input_length=700))\n",
    "m_lstm.add(LSTM(32))\n",
    "m_lstm.add(Dense(1,activation='sigmoid'))\n",
    "m_lstm.summary()\n",
    "\n",
    "m_lstm.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "m_lstm.fit(input_train,y_train,epochs=10,batch_size=128)\n",
    "\n",
    "train_loss_acc=m_lstm.evaluate(input_train,y_train)\n",
    "test_loss_acc=m_lstm.evaluate(input_test,y_test)\n",
    "print(train_loss_acc)\n",
    "print(test_loss_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 32)                6336      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 326,369\n",
      "Trainable params: 326,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 47s 232ms/step - loss: 0.6106 - acc: 0.6514\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 48s 244ms/step - loss: 0.3120 - acc: 0.8737\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 53s 272ms/step - loss: 0.2467 - acc: 0.9052\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 0.2199 - acc: 0.9155\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 51s 258ms/step - loss: 0.2021 - acc: 0.9243\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 48s 243ms/step - loss: 0.1725 - acc: 0.9367\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 55s 280ms/step - loss: 0.1675 - acc: 0.9387\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 66s 337ms/step - loss: 0.1442 - acc: 0.9494\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 49s 252ms/step - loss: 0.1381 - acc: 0.9521\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 48s 244ms/step - loss: 0.1336 - acc: 0.9527\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0944 - acc: 0.9690\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.3719 - acc: 0.8682\n",
      "[0.0943845734000206, 0.968999981880188]\n",
      "[0.37187618017196655, 0.8682000041007996]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "m_gru=Sequential()\n",
    "m_gru.add(Embedding(max_features,32))\n",
    "m_gru.add(GRU(32))\n",
    "m_gru.add(Dense(1,activation='sigmoid'))\n",
    "m_gru.summary()\n",
    "\n",
    "m_gru.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "m_gru.fit(input_train,y_train,epochs=10,batch_size=128)\n",
    "\n",
    "train_loss_acc=m_gru.evaluate(input_train,y_train)\n",
    "test_loss_acc=m_gru.evaluate(input_test,y_test)\n",
    "print(train_loss_acc)\n",
    "print(test_loss_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN 모형 반복\n",
    "- 2개의LSTM 모형 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 700, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 700, 32)           8320      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 353,217\n",
      "Trainable params: 353,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 1216s 6s/step - loss: 0.5861 - acc: 0.6640\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 284s 1s/step - loss: 0.3291 - acc: 0.8658\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 308s 2s/step - loss: 0.2767 - acc: 0.8944\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 302s 2s/step - loss: 0.2402 - acc: 0.9100\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 309s 2s/step - loss: 0.2182 - acc: 0.9174\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 317s 2s/step - loss: 0.2081 - acc: 0.9228\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 301s 2s/step - loss: 0.1946 - acc: 0.9309\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 319s 2s/step - loss: 0.1878 - acc: 0.9310\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 300s 2s/step - loss: 0.1640 - acc: 0.9399\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 314s 2s/step - loss: 0.1598 - acc: 0.9429\n",
      "782/782 [==============================] - 77s 98ms/step - loss: 0.1405 - acc: 0.9479\n",
      "782/782 [==============================] - 85s 108ms/step - loss: 0.3481 - acc: 0.8720\n",
      "[0.14053474366664886, 0.9478800296783447]\n",
      "[0.3481164872646332, 0.8720399737358093]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU, Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features,32, input_length=700))\n",
    "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.fit(input_train,y_train,epochs=10,batch_size=128)\n",
    "train_loss_acc=model.evaluate(input_train,y_train)\n",
    "test_loss_acc=model.evaluate(input_test,y_test)\n",
    "print(train_loss_acc)\n",
    "print(test_loss_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
